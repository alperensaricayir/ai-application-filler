# ğŸš€ Ollama GPU KullanÄ±mÄ± Rehberi

## âœ… Otomatik GPU AlgÄ±lama

Ollama **varsayÄ±lan olarak GPU'yu otomatik algÄ±lar ve kullanÄ±r**. Ã–zel bir ayar yapmanÄ±za gerek yok!

### GPU Durumunu Kontrol Etme

Terminal'de ÅŸu komutla Ollama'nÄ±n GPU kullanÄ±p kullanmadÄ±ÄŸÄ±nÄ± gÃ¶rebilirsiniz:

```powershell
# Ollama'yÄ± baÅŸlatÄ±n
ollama serve

# BaÅŸka bir terminal'de model Ã§alÄ±ÅŸtÄ±rÄ±n
ollama run llama3:8b

# GPU kullanÄ±mÄ±nÄ± gÃ¶rmek iÃ§in (NVIDIA GPU)
nvidia-smi
```

---

## ğŸ”§ GPU KullanÄ±mÄ±nÄ± Zorlamak (NVIDIA)

### 1. NVIDIA GPU Drivers GÃ¼ncel Mi?

```powershell
nvidia-smi
```

Bu komut GPU bilgilerini gÃ¶stermelidir. GÃ¶stermiyorsa driver gÃ¼ncelleyin:
- [NVIDIA Driver Ä°ndirme](https://www.nvidia.com/Download/index.aspx)

### 2. CUDA Kurulu Mu?

Ollama iÃ§in CUDA ayrÄ± kurmanÄ±za **gerek yok** - Ollama kendi CUDA runtime'Ä±nÄ± iÃ§erir.

---

## âš™ï¸ Environment Variables (Ä°steÄŸe BaÄŸlÄ±)

Ollama'nÄ±n davranÄ±ÅŸÄ±nÄ± kontrol etmek iÃ§in:

### GPU KullanÄ±mÄ±nÄ± Zorla
```powershell
# PowerShell'de kalÄ±cÄ± olarak ayarla
[System.Environment]::SetEnvironmentVariable('OLLAMA_GPU', '1', 'User')
```

### GPU KatmanlarÄ±nÄ± Ayarla (Bellek Optimizasyonu)
```powershell
# TÃ¼m modeli GPU'da Ã§alÄ±ÅŸtÄ±r (varsayÄ±lan)
[System.Environment]::SetEnvironmentVariable('OLLAMA_NUM_GPU', '999', 'User')

# Sadece belirli katman sayÄ±sÄ±nÄ± GPU'da Ã§alÄ±ÅŸtÄ±r (dÃ¼ÅŸÃ¼k VRAM iÃ§in)
[System.Environment]::SetEnvironmentVariable('OLLAMA_NUM_GPU', '30', 'User')
```

### Bellek Limitini Ayarla
```powershell
# GPU VRAM limitini ayarla (Ã¶rn: 8GB)
[System.Environment]::SetEnvironmentVariable('OLLAMA_GPU_MEMORY', '8192', 'User')
```

**NOT:** Environment variable'larÄ± ayarladÄ±ktan sonra **Ollama'yÄ± yeniden baÅŸlatÄ±n**.

---

## ğŸ–¥ï¸ AMD GPU (ROCm) DesteÄŸi

AMD GPU kullanÄ±yorsanÄ±z:

1. Ollama'nÄ±n AMD ROCm destekli versiyonunu indirin
2. ROCm drivers kurulu olmalÄ±
3. Set environment variable:

```powershell
[System.Environment]::SetEnvironmentVariable('OLLAMA_GPU_DRIVER', 'rocm', 'User')
```

---

## ğŸ macOS Metal DesteÄŸi

macOS'ta Apple Silicon (M1/M2/M3) iÃ§in:

```bash
# Metal otomatik kullanÄ±lÄ±r, kontrol iÃ§in:
export OLLAMA_GPU_DRIVER=metal
```

---

## ğŸ“Š Performans KontrolÃ¼

### Model YÃ¼klenirken GPU KullanÄ±mÄ±nÄ± Ä°zle

```powershell
# Terminal 1: Ollama Ã§alÄ±ÅŸtÄ±r
ollama run llama3:8b "Tell me about GPU acceleration"

# Terminal 2: GPU izle (NVIDIA)
nvidia-smi -l 1  # Her 1 saniyede gÃ¼ncelle
```

**Beklenen Ã‡Ä±ktÄ±:**
- **GPU Memory Used:** ArtmalÄ± (model yÃ¼klenince)
- **GPU Utilization:** %0-100 arasÄ± olmalÄ± (inference sÄ±rasÄ±nda)

---

## ğŸ” Sorun Giderme

### Problem: GPU KullanÄ±lmÄ±yor (CPU'da Ã‡alÄ±ÅŸÄ±yor)

**Ã‡Ã¶zÃ¼m 1:** Ollama'yÄ± yeniden baÅŸlatÄ±n
```powershell
# Ollama servisini durdur
taskkill /IM ollama.exe /F

# Yeniden baÅŸlat
ollama serve
```

**Ã‡Ã¶zÃ¼m 2:** CUDA kontrol
```powershell
# NVIDIA Control Panel > System Information > Components
# "3D Settings" altÄ±nda CUDA olmalÄ±
```

**Ã‡Ã¶zÃ¼m 3:** Ollama loglarÄ±nÄ± kontrol
```powershell
# Ollama serve Ã§Ä±ktÄ±sÄ±nda ÅŸuna benzer satÄ±r olmalÄ±:
# "NVIDIA GPU detected" veya "Loaded GPU driver"
```

### Problem: "Out of Memory" HatasÄ±

**Ã‡Ã¶zÃ¼m:** Daha kÃ¼Ã§Ã¼k model kullanÄ±n veya GPU katmanlarÄ±nÄ± azaltÄ±n
```powershell
# Katman sayÄ±sÄ±nÄ± dÃ¼ÅŸÃ¼r
[System.Environment]::SetEnvironmentVariable('OLLAMA_NUM_GPU', '20', 'User')

# Ollama'yÄ± yeniden baÅŸlat
taskkill /IM ollama.exe /F
ollama serve
```

---

## ğŸ¯ Bu Proje Ä°Ã§in Ollama YapÄ±landÄ±rmasÄ±

Backend zaten Ollama'yÄ± `http://localhost:11435` Ã¼zerinden kullanÄ±yor.

### Kontrol AdÄ±mlarÄ±:

1. **Ollama Ã‡alÄ±ÅŸÄ±yor Mu?**
```powershell
curl http://localhost:11435/api/tags
```

2. **Model Ä°ndirilmiÅŸ Mi?**
```powershell
ollama list
```

3. **GPU Test**
```powershell
ollama run llama3:8b "Test GPU performance with a complex question"
```

4. **Admin Panel'den Kontrol**
- http://localhost:5174/admin
- Provider: OLLAMA seÃ§
- Model: llama3:8b seÃ§
- "Test" butonuna bas

---

## ğŸ’¡ Ã–neriler

### VRAM'e GÃ¶re Model SeÃ§imi:

- **4GB VRAM:** `llama3:8b` (quantized)
- **6GB VRAM:** `llama3:8b` veya `mistral:7b`
- **8GB+ VRAM:** `llama3:8b`, `mistral:7b`, veya `mixtral:8x7b`
- **12GB+ VRAM:** `llama3:70b` (quantized)

### Performans Ä°puÃ§larÄ±:

1. **Sadece bir model aktif tutun** - Bellekten tasarruf
2. **prompt_cache kullanÄ±n** - Backend zaten yapÄ±yor âœ…
3. **Batch processing kullanÄ±n** - PromptBuilder zaten destekliyor âœ…

---

## âœ… SonuÃ§

Ollama **otomatik olarak GPU kullanÄ±r**. Sadece:

1. GPU driver'larÄ±n gÃ¼ncel olmalÄ±
2. Ollama Ã§alÄ±ÅŸÄ±yor olmalÄ± (`ollama serve`)
3. Model indirilmiÅŸ olmalÄ± (`ollama pull llama3:8b`)

**ArtÄ±k hazÄ±rsÄ±nÄ±z!** ğŸš€

Test iÃ§in: http://localhost:5174/ â†’ Generate Answer butonu
